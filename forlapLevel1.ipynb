{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from socket import error as SocketError\n",
    "import urllib\n",
    "import pandas\n",
    "import MySQLdb\n",
    "import re\n",
    "import time, random\n",
    "\n",
    "start_time = time.time()\n",
    "db = MySQLdb.connect(\n",
    "    host = '127.0.0.1',\n",
    "    user = 'root',\n",
    "    passwd = '310116',\n",
    "    db = 'diktiScraperdb'\n",
    ")\n",
    "class scrape(object): #membuat objek scrape\n",
    "\n",
    "    def __init__(self, nameOfMachine): #inisiasi objek\n",
    "        self.nameOfMachine = nameOfMachine #pemberian atribut objek pada inisiasi\n",
    "\n",
    "    def getLink(self, addr): #membuat fungsi getLink\n",
    "        dataLink = {  #membuat dictionary\n",
    "        'addt' : [], #teks untuk penambahan\n",
    "        'link' : [] #link yang ada di halaman\n",
    "        }\n",
    "        def scrapePage(addr):\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try :\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                addtText = \"\"\n",
    "                for recordLink in recordRow.findAll('a', href=True):\n",
    "                    tempData = recordLink.text.split()\n",
    "                    for listOfTempData in tempData:\n",
    "                        addtText = addtText + listOfTempData + \" \"\n",
    "                    dataLink['addt'].append(str(addtText))\n",
    "                    dataLink['link'].append(str(recordLink['href']))\n",
    "            print \"Get DataLink Scrape HTML Object\"        \n",
    "            return dataLink\n",
    "        except AttributeError :\n",
    "            print \"Not Found\"\n",
    "            return dataLink\n",
    "    def getTextOnly(self,addr):\n",
    "        dataText = {\n",
    "            'code' : [],\n",
    "            'univ' : []\n",
    "        }\n",
    "        listCode = []\n",
    "        listUniv = []\n",
    "        def scrapePage(addr):\n",
    "            wt = random.uniform(2, 5)\n",
    "            time.sleep(wt)\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try:\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                for recordText in recordRow.findAll('td'):\n",
    "                    listCode.append(recordText.text)\n",
    "                    listUniv.append(recordText.text)\n",
    "            for item in listCode[1::19]:\n",
    "                addtText = \"\"\n",
    "                tempData = item.split()\n",
    "                for listOfTempData in tempData:\n",
    "                    addtText = addtText + listOfTempData + \" \"\n",
    "                dataText['code'].append(str(addtText))\n",
    "                #print addtText\n",
    "            for item in listUniv[2::19]:\n",
    "                addtText = \"\"\n",
    "                tempData = item.split()\n",
    "                for listOfTempData in tempData:\n",
    "                    addtText = addtText + listOfTempData + \" \"\n",
    "                dataText['univ'].append(str(addtText))\n",
    "                #print addtText \n",
    "            print \"Get Text ScrapeHTML Object\"\n",
    "            #print dataText\n",
    "            return dataText\n",
    "        except SocketError as e:\n",
    "            if e.errno != errno.ECONNRESET:\n",
    "                raise # Not error we are looking for\n",
    "            pass # Handle error here.\n",
    "\n",
    "urlServer = \"/home/alien/diktiScraper\" #server program host\n",
    "urlTarget = \"http://forlap.dikti.go.id/perguruantinggi/homerekap\" #home rekap\n",
    "scraperLayer0 = scrape('layer0_df')\n",
    "dataGet = scraperLayer0.getLink(urlTarget)\n",
    "layer0_df = pandas.DataFrame(dataGet, columns=['addt', 'link'])\n",
    "layer0_df.index.name = 'institutionId'\n",
    "cur = db.cursor()\n",
    "query0 = 'USE diktiScraperdb;'\n",
    "cur.execute(query0)\n",
    "indexUniv = \"indexUniv\"\n",
    "query7 = \"DROP TABLE IF EXISTS  \"+indexUniv+\";\"\n",
    "query8 = \"CREATE TABLE \"+indexUniv+\" (Id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,code VARCHAR(255),inst VARCHAR(255), univ VARCHAR(255), link VARCHAR(255), valid CHAR(1));\"\n",
    "cur.execute(query7)\n",
    "cur.execute(query8)\n",
    "l = 0\n",
    "i = 0\n",
    "for link in layer0_df['link']:\n",
    "    urlTarget0 = link\n",
    "    print str(i) + \" \" + layer0_df['addt'][i]\n",
    "    if i == 0 or i > 2 :\n",
    "        scraper1 = scrape('df1')\n",
    "        dataGet1 = scraper1.getTextOnly(urlTarget0)\n",
    "        df1 = pandas.DataFrame(dataGet1, columns=['code', 'univ'])\n",
    "        df1.index.name = 'institutionId'\n",
    "        j = 0\n",
    "        for univ in df1['univ']:\n",
    "            a = df1['code'][j]\n",
    "            a = re.sub(\"[!@#$/']\", '', a)\n",
    "            b = layer0_df['addt'][i] #inst\n",
    "            b = re.sub(\"[!@#$/']\", '', b)\n",
    "            c = univ #univ\n",
    "            c = re.sub(\"[!@#$/']\", '', c)\n",
    "            print \" > \" + a + \" | \" + b + \" | \" + c\n",
    "            query11 = \"INSERT INTO indexUniv (code, inst, univ) VALUES ('\"+a+\"','\"+b+\"','\"+c+\"');\"\n",
    "            cur.execute(query11)\n",
    "            db.commit()\n",
    "            j = j + 1\n",
    "    elif i == 2:\n",
    "        #\n",
    "        scraper1 = scrape('df1')\n",
    "        dataGet1 = scraper1.getLink(urlTarget0)\n",
    "        df1 = pandas.DataFrame(dataGet1, columns=['addt', 'link'])\n",
    "        df1.index.name = 'institutionId'\n",
    "        j = 0\n",
    "        for link1 in  df1['link']:\n",
    "            urlTarget1 = link1\n",
    "            scraper2 = scrape('df2')\n",
    "            dataGet2 = scraper2.getLink(urlTarget1)\n",
    "            df2 = pandas.DataFrame(dataGet2, columns=['addt', 'link'])\n",
    "            df2.index.name = 'institutionId'\n",
    "            k = 0\n",
    "            for link2 in df2['link']:\n",
    "                urlTarget2 = link2\n",
    "                scraper3 = scrape('df3')\n",
    "                dataGet3 = scraper3.getTextOnly(urlTarget2)\n",
    "                df3 = pandas.DataFrame(dataGet3, columns=['code', 'univ'])\n",
    "                df3.index.name = 'institutionId'\n",
    "                l = 0\n",
    "                for univ in df3['univ']:\n",
    "                    a = df3['code'][l] #code\n",
    "                    a = re.sub(\"[!@#$/']\", '', a)\n",
    "                    b = layer0_df['addt'][i] #inst\n",
    "                    b = re.sub(\"[!@#$/']\", '', b)\n",
    "                    c = df1['addt'][j] #inst\n",
    "                    c = re.sub(\"[!@#$/']\", '', c)\n",
    "                    d = df2['addt'][k] #inst\n",
    "                    d = re.sub(\"[!@#$/']\", '', d)\n",
    "                    d = b + \" \" + c + \" \" + d\n",
    "                    e = univ\n",
    "                    e = re.sub(\"[!@#$/']\", '', e)\n",
    "                    print \" > \" + a + \" | \" + d + \" | \" + e\n",
    "                    query11 = \"INSERT INTO indexUniv (code, inst, univ) VALUES ('\"+a+\"','\"+d+\"','\"+e+\"');\"\n",
    "                    cur.execute(query11)\n",
    "                    db.commit()\n",
    "                    l = l+1\n",
    "                k = k+1\n",
    "            j = j+1\n",
    "    elif i == 1 :\n",
    "        scraper1 = scrape('df1')\n",
    "        dataGet1 = scraper1.getLink(urlTarget0)\n",
    "        df1 = pandas.DataFrame(dataGet1, columns=['addt', 'link'])\n",
    "        df1.index.name = 'institutionId'\n",
    "        j = 0\n",
    "        for link1 in  df1['link']:\n",
    "            urlTarget1 = link1\n",
    "            scraper2 = scrape('df2')\n",
    "            dataGet2 = scraper2.getTextOnly(urlTarget1)\n",
    "            df2 = pandas.DataFrame(dataGet2, columns=['code', 'univ'])\n",
    "            df2.index.name = 'institutionId'\n",
    "            k = 0\n",
    "            for univ in df2['univ']:\n",
    "                a = df2['code'][k] #code\n",
    "                a = re.sub(\"[!@#$/']\", '', a)\n",
    "                b = layer0_df['addt'][i] #inst\n",
    "                b = re.sub(\"[!@#$/']\", '', b)\n",
    "                c = df1['addt'][j] #inst\n",
    "                c = re.sub(\"[!@#$/']\", '', c)\n",
    "                c = b + \" \" + c\n",
    "                d = univ #univ\n",
    "                d = re.sub(\"[!@#$/']\", '', d)\n",
    "                print \" > \" + a + \" | \" + c + \" | \" + d\n",
    "                query11 = \"INSERT INTO indexUniv (code, inst, univ) VALUES ('\"+a+\"','\"+c+\"','\"+d+\"');\"\n",
    "                cur.execute(query11)\n",
    "                db.commit()\n",
    "                k = k+1\n",
    "            j = j+1\n",
    "    i = i+1\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
